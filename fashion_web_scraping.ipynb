{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcBtAo_4l4b1"
      },
      "source": [
        "##Necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyWrlC6Uuftd",
        "outputId": "705211e4-6001-4129-b800-d182e212a816"
      },
      "source": [
        "!pip3 install newspaper3k\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "  Using cached https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl\n",
            "Collecting feedparser>=5.2.1\n",
            "  Using cached https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl\n",
            "Collecting tldextract>=2.0.1\n",
            "  Using cached https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02/tinysegmenter-0.3-cp37-none-any.whl\n",
            "Collecting cssselect>=0.9.2\n",
            "  Using cached https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.1)\n",
            "Processing /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed/feedfinder2-0.0.4-cp37-none-any.whl\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Processing /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34/jieba3k-0.35.1-cp37-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a/sgmllib3k-1.0.0-cp37-none-any.whl\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n",
            "Collecting requests-file>=1.4\n",
            "  Using cached https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
            "Installing collected packages: sgmllib3k, feedparser, requests-file, tldextract, tinysegmenter, cssselect, feedfinder2, jieba3k, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zyMnejWl80t"
      },
      "source": [
        "**n** - number of next pages from single web page\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhENn90fsFcu"
      },
      "source": [
        "n = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2taNzZ19UND_"
      },
      "source": [
        "I have picked three online fashion magazines **ELLE**, **GLAMOUR** and **AVANTI** and scrapped their web pages for fashion articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy94xYCl6ydn"
      },
      "source": [
        "##ELLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzqxRCFBKA0M"
      },
      "source": [
        "elle = \"https://www.elle.pl/moda?page=2\"\n",
        "elle_format = \"https://www.elle.pl/moda{}\"\n",
        "home_format = \"https://www.elle.pl{}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1eBDGsPwRwFo"
      },
      "source": [
        "\n",
        "elle_links = []\n",
        "elle_text = ''\n",
        "\n",
        "page = requests.get(elle)\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "for i in range(n):\n",
        "\n",
        "  links = [div.a for div in\n",
        "        soup.findAll('div', attrs={'class' : \" col-12 col-md-6 col-lg-6 col-xl-6 box-all-link d-flex flex-column \"})]\n",
        "  for link in links:\n",
        "\n",
        "      elle_links.append(elle_format.format(link['href']))\n",
        "      elle_article = Article(elle_format.format(link['href']), language=\"pl\")\n",
        "      elle_article.download()\n",
        "\n",
        "      try:\n",
        "        elle_article.parse()\n",
        "        elle_text += elle_article.text.strip()\n",
        "      except :\n",
        "        pass\n",
        "\n",
        "  next_page = [span.a for span in\n",
        "        soup.findAll('span', attrs={'class' : \"next\"})]\n",
        "  next_link = requests.get(home_format.format(next_page[0]['href']))\n",
        "  soup = BeautifulSoup(next_link.content, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y868V2coVXi"
      },
      "source": [
        "##GLAMOUR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_RaHtUhWoH0t"
      },
      "source": [
        "glamour = 'https://www.glamour.pl/tag/moda'\n",
        "glamour_format = 'https://www.glamour.pl{}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i8W2t_PxoUCT"
      },
      "source": [
        "glamour_links = []\n",
        "glamour_text = ''\n",
        "\n",
        "\n",
        "page = requests.get(glamour)\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "for i in range(n):\n",
        "  for link in soup.findAll('a', {'class': \"d-flex flex-column box-all-link\"}):\n",
        "    if re.findall('/artykul/', link.get('href')) != []:\n",
        "      glamour_links.append(glamour_format.format(link.get('href')))\n",
        "      glamour_article = Article(glamour_format.format(link['href']), language=\"pl\")\n",
        "      glamour_article.download()\n",
        "      try:\n",
        "        glamour_article.parse()\n",
        "        glamour_text += glamour_article.text.strip()\n",
        "      except :\n",
        "        pass\n",
        "\n",
        "  next_link = soup.findAll('span', attrs={'class': \"next\"})[0].findAll('a')[0].get('href')\n",
        "  next_page = requests.get(glamour_format.format(next_link))\n",
        "  soup = BeautifulSoup(next_page.content, 'html.parser')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_N0H7oM7VG"
      },
      "source": [
        "##AVANTI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "REU_rK3EtrIo"
      },
      "source": [
        "avanti = 'https://avanti24.pl/Magazyn/0,150441.html'\n",
        "avanti_format = 'https://avanti24.pl{}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOQTCgQxa0Ds"
      },
      "source": [
        "avanti_links = []\n",
        "avanti_text = ''\n",
        "\n",
        "\n",
        "page = requests.get(avanti)\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "for i in range(n):\n",
        "  links = [div.a for div in soup.findAll('div', attrs={'class' : \"imgw\"})]\n",
        "  for link in links:\n",
        "    if link != None and re.findall('/Magazyn/', link['href']) != [] and link['href'] not in avanti_links:\n",
        "      avanti_links.append(link['href'])\n",
        "      avanti_article = Article(link['href'], language=\"pl\")\n",
        "      avanti_article.download()\n",
        "      try:\n",
        "        avanti_article.parse()\n",
        "        avanti_text += avanti_article.text.strip()\n",
        "      except :\n",
        "        pass\n",
        "\n",
        "  next_page = requests.get(avanti_format.format(soup.findAll('a', {'class': 'next'})[0]['href']))\n",
        "  soup = BeautifulSoup(next_page.content, 'html.parser')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoHzTF8hqW-F"
      },
      "source": [
        "data = elle_text + glamour_text + avanti_text # combined dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhd3qP2s0xxA"
      },
      "source": [
        "len(set(data.split())) # number of tokens in data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6PtvMNNxu-q"
      },
      "source": [
        "path = '/content/drive/MyDrive/'\n",
        "textfile = open(\"fashion_corpus.txt\", 'w')\n",
        "textfile.write(data)\n",
        "textfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}